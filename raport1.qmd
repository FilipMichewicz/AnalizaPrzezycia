---
title: "Analiza Przeżycia"
subtitle: "Raport 1"
author:
  - "Wiktor Niedźwiedzki (258882)"
  - "Filip Michewicz (282239)"
date: last-modified
date-format: "D [listopada] YYYY [Anno Domini]"

lang: pl
jupyter: python3

format:
  pdf:
    number-sections: true
    fig-cap-location: bottom
    tbl-cap-location: top
    toc: true
    lof: true
    lot: true

execute:
  echo: true

crossref:
  fig-title: "Wykres"
  tbl-title: "Tabela"
  eq-title: "Równanie"
  lof-title: "Spis rysunków"
  lot-title: "Spis tabel"

header-includes:
  - \usepackage{fontspec}
  - \usepackage{polyglossia}
  - \setdefaultlanguage{polish}
  - \usepackage{graphicx}
  - \usepackage{float}
  - \usepackage{xcolor}
  - \usepackage{amsmath}
  - \renewcommand{\contentsname}{Spis treści}
  - \renewcommand{\listfigurename}{Spis wykresów}
  - \renewcommand{\listtablename}{Spis tabel}
  - \renewcommand{\figurename}{Wykres}
  - \renewcommand{\tablename}{Tabela}
  - \definecolor{ForestGreen}{rgb}{0.1333, 0.5451, 0.1333}
  - \definecolor{SteelBlue}{rgb}{0.2745, 0.5098, 0.7059}
  - \definecolor{Tomato}{rgb}{1.0, 0.3882, 0.2784}
---

```{python}
#| echo: false

# Wczytywanie bibliotek

from math import log, e, factorial, floor, comb
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from random import seed
from scipy.integrate import quad as integrate

#Narazie nieużywane
from scipy.stats import gamma, binomtest
from statsmodels.stats.proportion import proportion_confint

from IPython.display import Markdown  # formatowanie zmiennych do markdown
from tabulate import tabulate  # formatowanie tabelek

seed(6*10**6)  # dla powtarzalności wyników
```

# Lista 1

Lista pierwsza obejmuje analizę rozszerzonego rozkładu Weibulla $\left( \mathcal{EW} \left( \alpha, \beta, \gamma \right)\right)$: definicje jego funkcji, generowanie danych, wizualizację oraz porównanie statystyk empirycznych i teoretycznych.

## Zadanie 1

W tym zadaniu definiujemy funkcje:

* gęstości,
* dystrybuanty,
* kwantylową,
* hazardu

dla rozkładu $\mathcal{EW} \left( \alpha, \beta, \gamma \right)$.

### Funkcja gęstości

$$
f(x) = \frac{\alpha \gamma}{\beta} \left( \frac{x}{\beta} \right)^{\alpha-1}
\left( 1 - exp \left( - \left( \frac{x}{\beta} \right)^\alpha \right) \right)^{\gamma-1}
exp \left( - \left( \frac{x}{\beta} \right)^\alpha \right)  \mathbf{1}_{\left(0;\infty\right)}\left( x \right)
$$

```{python}
def density(x, alpha, beta, gamma):
    return(alpha*gamma/beta*(x/beta)**(alpha-1)*
           (1-e**(-(x/beta)**alpha))**(gamma-1)*
           e**(-(x/beta)**alpha))
```

### Dystrybuanta

$$
F(t) = 
\left( 1 - exp \left( - \left( \frac{x}{\beta} \right)^\alpha \right) \right)^\gamma
\mathbf{1}_{\left(0;\infty\right)}\left( t \right)
$$

```{python}
def distribution(t, alpha, beta, gamma):
    return((1-e**(-(t/beta)**alpha))**gamma)
```

### Funkcja kwantylowa (dystrybuanta odwrotna)

$$
Q(p) = \beta \left( -\ln \left( 1-p^{\frac{1}{\gamma}} \right) \right)^{\frac{1}{\alpha}}
$$

```{python}
def quantile(p, alpha, beta, gamma):
    return(beta*(-log(1-p**(1/gamma)))**(1/alpha))
```

### Funkcja hazardu

$$
h(x) = \frac{f(x)}{1-F(x)} =
\frac{\alpha \gamma \left( \frac{x}{\beta} \right)^{\alpha-1}
\left( 1 - exp \left( - \left( \frac{x}{\beta} \right)^\alpha \right) \right)^{\gamma-1}
exp \left( - \left( \frac{x}{\beta} \right)^\alpha \right)}{\beta \left(1-\left( 1 - exp \left( - \left( \frac{x}{\beta} \right)^\alpha \right) \right)^\gamma \right)}
\mathbf{1}_{\left(0;\infty\right)}\left( x \right)
$$

```{python}
def hazard(x, alpha, beta, gamma):
    return(density(x, alpha, beta, gamma) / 
           (1-distribution(x, alpha, beta, gamma)))
```

## Zadanie 2

Poniżej przedstawione są wykresy przykładowych funkcji hazardu.

```{python}
#| echo: false
#| fig-align: center
#| fig-cap: Przykładowe funkcje hazardu dla pięciu trójek parametrów

x = np.linspace(0.001, 8, 1000)

plt.ylim(0, 1.2)
plt.xlim(0, 8)

plt.plot(x, hazard(x, 1/2, 2, 3/4), color="dodgerblue",
         label=r"$\alpha = \frac{1}{2}, \ \beta = 2, \ \gamma = \frac{3}{4}$")

plt.plot(x, hazard(x, 1, 2, 1), color="red",
         label=r"$\alpha = 1, \ \beta = 2, \ \gamma = 1$")

plt.plot(x, hazard(x, 3/2, 3, 3), color="green",
         label=r"$\alpha = \frac{3}{2}, \ \beta = 3, \ \gamma = 3$")

plt.plot(x, hazard(x, 3/2, 4, 1/8), color="gold",
         label=r"$\alpha = \frac{3}{2}, \ \beta = 4, \ \gamma = \frac{1}{8}$")

plt.plot(x, hazard(x, 3/4, 1, 2), color="darkorchid",
         label=r"$\alpha = \frac{3}{4}, \ \beta = 1, \ \gamma = 2$")

plt.legend()
plt.grid(alpha=0.3)

plt.show()
plt.close()
```

## Zadanie 3

W tym zadaniu piszemy funkcję generującą zmienne z rozkładu $\mathcal{EW} \left( \alpha, \beta, \gamma \right)$. Ze względu na to, że łatwo jest wyznaczyć funkcję odwrotną do dystrybuanty (inaczej: funkcję kwantylową), wykorzystamy ją do generowania danych w następujący sposób:

1. $U_i \sim Uniform \left( 0, 1 \right)$
2. $X_i = F^{-1}(U_i)$

Wtedy zmienna losowa $X_i$ ma rozkład jednoznacznie określony przez dystrybuantę $F(t)$.

```{python}
def generator(alpha, beta, gamma):
    return(quantile(np.random.uniform(), alpha, beta, gamma))
    # [0;1) to opcja domyślna
```

## Zadanie 4

W tym zadaniu będziemy generować odpowiednio $n$=50 oraz $n$=100 próbek z dwóch trójek parametrów rozkładu $\mathcal{EW} \left( \alpha, \beta, \gamma \right)$, oraz przedstawiać ich gęstości na rysunku porównując z rzeczywistymi.

Dla ułatwienia piszemy funkcję generującą coś tam i chuj.

```{python}
def ew(n, alpha, beta, gamma):

    dane = []
    y_max = 0
    
    for i in range(len(n)):
        
        x = np.array([generator(alpha, beta, gamma) for _ in range(n[i])])
        counts, bin_edges = np.histogram(x, bins="scott", density=True)
        y_max = max(y_max, counts.max())
        dane.append(x)

    fig, ax = plt.subplots(nrows=1, ncols=len(n), figsize=(5*len(n), 5))
    
    for i in range(len(n)):
        
        x_min, x_max = min(dane[i]), max(dane[i])
        x_density = np.linspace(x_min, x_max, 1000)
        
        ax[i].hist(x, bins="scott", density = True,
                   color="dodgerblue", alpha=0.5, label="Histogram")
        ax[i].plot(x_density, density(x_density, alpha, beta, gamma),
                   color="red", label="Gęstość")
        ax[i].set_xlim(x_min, x_max)
        ax[i].set_ylim(0, y_max+0.02)
        ax[i].set_title(f"Liczba punktów: n={n[i]}")
        ax[i].grid(alpha=0.3)
        ax[i].legend(loc="upper right")

        dane.append(x)

    plt.tight_layout()
    plt.show()

    return(dane)
```

```{python}
#| echo: false
#| fig-cap: Generowanie 50 oraz 100 próbek z rozkładu $\mathcal{EW} \left( \alpha = 2, \beta = 1, \gamma = 1 \right)$
#| fig-align: center

par1 = ew((50, 100), 2, 1, 1)
```

```{python}
#| echo: false
#| fig-cap: Generowanie 50 oraz 100 próbek z rozkładu $\mathcal{EW} \left( \alpha = 1.5, \beta = 3, \gamma = 3 \right)$
#| fig-align: center

par2 = ew((50, 100), 1.5, 3, 3)
```

## Zadanie 5

Porównanie statystyk opisowych danych wygenerowanych oraz teoretycznych bla bla bla.

```{python}
#| echo: false
#| tbl-cap: AAAAAAAAAAAAAAAAAAAAAAAAAA

stat_50_1 = [
    np.min(par1[0]),
    np.percentile(par1[0], 25),
    np.median(par1[0]),
    np.percentile(par1[0], 75),
    np.max(par1[0]),
    np.mean(par1[0]),
    np.std(par1[0], ddof=1)
    ]
stat_50_1 = [round(x, 3) for x in stat_50_1]  
    
stat_100_1 = [
    np.min(par1[1]),
    np.percentile(par1[1], 25),
    np.median(par1[1]),
    np.percentile(par1[1], 75),
    np.max(par1[1]),
    np.mean(par1[1]),
    np.std(par1[1], ddof=1)
    ]
stat_100_1 = [round(x, 3) for x in stat_100_1]    

stat_theoretical_1 = [
    "-",
    quantile(0.25, 2, 1, 1),
    quantile(0.5, 2, 1, 1),
    quantile(0.75, 2, 1, 1),
    "-",
    integrate(lambda x: x*density(x, 2, 1, 1), 0, quantile(0.999, 2, 1, 1))[0],
    integrate(lambda x: x**2*density(x, 2, 1, 1), 0, quantile(0.999, 2, 1, 1))[0] - (integrate(lambda x: x*density(x, 2, 1, 1), 0, quantile(0.999, 2, 1, 1))[0])**2
    ]
stat_theoretical_1 = [round(x, 3) if isinstance(x, (int, float)) else x for x in stat_theoretical_1]    

stat_df_1 = pd.DataFrame(
    [stat_50_1, stat_100_1, stat_theoretical_1],
    columns=["Min", "Pierwszy Kw.", "Mediana", "Trzeci Kw.", "Max", "Średnia", "Odch.Stand."],
    index=["$n$=50", "$n$=100", "Dane teoretyczne"]
)

Markdown( 
  tabulate(stat_df_1, showindex=False, 
           headers=stat_df_1.columns))
```

```{python}
#| echo: false
#| tbl-cap: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

stat_50_2 = [
    np.min(par2[0]),
    np.percentile(par2[0], 25),
    np.median(par2[0]),
    np.percentile(par2[0], 75),
    np.max(par2[0]),
    np.mean(par2[0]),
    np.std(par2[0], ddof=1)
    ]
stat_50_2 = [round(x, 3) for x in stat_50_2]  
    
stat_100_2 = [
    np.min(par2[1]),
    np.percentile(par2[1], 25),
    np.median(par2[1]),
    np.percentile(par2[1], 75),
    np.max(par2[1]),
    np.mean(par2[1]),
    np.std(par2[1], ddof=1)
    ]
stat_100_2 = [round(x, 3) for x in stat_100_2]    

stat_theoretical_2 = [
    "-",
    quantile(0.25, 1.5, 3, 3),
    quantile(0.5, 1.5, 3, 3),
    quantile(0.75, 1.5, 3, 3),
    "-",
    integrate(lambda x: x*density(x, 1.5, 3, 3), 0, quantile(0.999, 1.5, 3, 3))[0],
    integrate(lambda x: x**2*density(x, 1.5, 3, 3), 0, quantile(0.999, 1.5, 3, 3))[0] - (integrate(lambda x: x*density(x, 1.5, 3, 3), 0, quantile(0.999, 1.5, 3, 3))[0])**2
    ]
stat_theoretical_2 = [round(x, 3) if isinstance(x, (int, float)) else x for x in stat_theoretical_2]    

stat_df_2 = pd.DataFrame(
    [stat_50_2, stat_100_2, stat_theoretical_2],
    columns=["Min", "Pierwszy Kw.", "Mediana", "Trzeci Kw.", "Max", "Średnia", "Odch.Stand."],
    index=["$n$=50", "$n$=100", "Dane teoretyczne"]
)

Markdown( 
  tabulate(stat_df_2, showindex=False, 
           headers=stat_df_2.columns))
```

## Zadanie dodatkowe 1

Tu liczymy wariancje i wariancje, policzymy dla wcześniejszych rozkładów, jako iż losowanie punktów średnio wychodzi to użyjemy metody parabol, porównamy do scipy.integrate.quad

```{python}
def simpson_rule(a, b, function, n):

    if n%2 != 0: raise Exception("The number of intervals must be even.")
    
    x_points = np.linspace(a, b, n+1)
    y_points = []
    for i in x_points: y_points.append(function(i))
    
    h = (b-a)/(n)
    integral = 0
    integral += y_points[0]
    even = False
    
    for i in range(1, n):
        if even:
            integral += 2*y_points[i]
            even = False
        else:
            integral += 4*y_points[i]
            even = True

    integral += y_points[n]
    integral = h*integral/3
    
    return(integral)
```

```{python}
def expected_value(alpha, beta, gamma, moment = 1):
    return(simpson_rule(0, quantile(0.999, alpha, beta, gamma),
           lambda x: x**moment * density(x, alpha, beta, gamma), 10**5))
```

```{python}
def variance(alpha, beta, gamma):
    return(expected_value(alpha, beta, gamma, 2) -
           expected_value(alpha, beta, gamma)**2)
```

```{python}
#| echo: false
#| tbl-cap: Wartość oczekiwana dwóch trójek rozszerzonego rozkładu Weibulla

ev = [
  expected_value(2, 1, 1),
  expected_value(1.5, 3, 3)
]

scipy_ev = [
  integrate(lambda x: x*density(x, 2, 1, 1), 0, quantile(0.999, 2, 1, 1))[0],
  integrate(lambda x: x*density(x, 1.5, 3, 3), 0, quantile(0.999, 1.5, 3, 3))[0]
]

ev_df = pd.DataFrame(
    [ev, scipy_ev],
    columns=[r"$\alpha$ = 2, $\beta$ = 1, $\gamma$ = 1", r"$\alpha$ = 1.5, $\beta$ = 3, $\gamma$ = 3"],
    index=["Funkcja własna", "Funkcja wbudowana"]
).round(3)

Markdown( 
  tabulate(ev_df, showindex=False, 
           headers=ev_df.columns))
```

```{python}
#| echo: false
#| tbl-cap: Wariancja dwóch trójek rozszerzonego rozkładu Weibulla

v = [
  variance(2, 1, 1),
  variance(1.5, 3, 3)
]

scipy_v = [
  integrate(lambda x: x**2*density(x, 2, 1, 1), 0, quantile(0.999, 2, 1, 1))[0]-(integrate(lambda x: x*density(x, 2, 1, 1), 0, quantile(0.999, 2, 1, 1))[0])**2,
  integrate(lambda x: x**2*density(x, 1.5, 3, 3), 0, quantile(0.999, 1.5, 3, 3))[0]-(integrate(lambda x: x*density(x, 1.5, 3, 3), 0, quantile(0.999, 1.5, 3, 3))[0])**2
]

v_df = pd.DataFrame(
    [ev, scipy_ev],
    columns=[r"$\alpha$ = 2, $\beta$ = 1, $\gamma$ = 1", r"$\alpha$ = 1.5, $\beta$ = 3, $\gamma$ = 3"],
    index=["Funkcja własna", "Funkcja wbudowana"]
).round(3)

Markdown( 
  tabulate(v_df, showindex=False, 
           headers=v_df.columns))
```

## Zadanie dodatkowe 2

*Trzeba ręcznie policzyć pochodne, dowód na kartce, i rozważyć na przypadki*

# Lista 2

Siemanko, witam w mojej kuchni. Dzisiaj zajmiemy się generacją danych cenzorywhch prwaostroni I-go typu, II-go typu i losowe (niezależne). Zakładamy, że generowane dane są i.i.d.

## Zadanie 1

Na początek definiujemy funkcję kwantylową uogólnionego rozkładu wykładniczego $\mathcal{GE} \left( \lambda, \alpha \right)$, z którego będziemy generować dane.
Wzór na dystrybuantę: $F(t) = \left( 1-exp \left( \lambda t \right) \right)^\alpha$.
Wzór na funkcję kwantylową: $Q(p) = - \frac{log \left( 1 - p^\frac{1}{\alpha}\right)}{\lambda}$.
Łatwo widać, że $\mathcal{GE} \left( \lambda, \alpha \right) = \mathcal{EW} \left( 1, \frac{1}{\lambda}, \alpha \right)$.

```{python}
def quantile(p, alpha, lamba):
    return(-log(1-p**(1/alpha))/lamba)
```

Informację na temat danych cenzurowanych będziemy zawierać w tzw. indykatorze cenzurowania:

$$
\delta_i =
\begin{cases}
1, & \text{jeśli dane są kompletne (niecenzurowane)}, \\[3pt]
0, & \text{jeśli dane są niekompletne (cenzurowane)}.
\end{cases}
$$

### I. typ

Czyli mierzymy tylko do pewnego czasu $t_0$. Niech $X_i$ - zaobserwowane czasy życia. Definiujemy zmienne losowe cenzurowane jako:

$$
T_i =
\begin{cases}
X_i, & X_i \le t_0 \quad (\delta_i = 1),\\[3pt]
t_0, & X_i > t_0 \quad (\delta_i = 0),
\end{cases}
\quad
\text{czyli} \quad
T_i = \min(X_i,\, t_0).
$$

```{python}
def I(alpha, lamba, t0, n):
    x = np.array([quantile(np.random.uniform(), alpha, lamba)
                  for _ in range(n)])
    data = []
    for i in range(n):
        if x[i] <= t0: data.append((x[i], 1))
        else: data.append((t0, 0))
    return(data)
```

Przykładowe wywołanie:

```{python}
I(1, 2, 0.5, 10)
```

### II. typ

Czyli mierzymy tylko do uzyskania $m \leq n$ danyck kompletnych. Niech $X_{(k)}$ - $k$-ta statystyka pozycyjna. Definiujemy zmienne losowe cenzurowane jako:

$$
T_i =
\begin{cases}
X_{(i)}, & i = 1, \dots, m \quad (\delta_i = 1),\\[3pt]
X_{(m)}, & i = m+1, \dots, n \quad (\delta_i = 0),
\end{cases}
\quad
\text{czyli} \quad
T_i = \min \bigl(X_{(i)}, X_{(m)}\bigr).
$$

```{python}
def II(alpha, lamba, m, n):
    x = sorted(np.array([quantile(np.random.uniform(), alpha, lamba)
                         for _ in range(n)]))
    data = []
    for i in range(m):
        data.append((x[i], 1))
    for j in range(n-m):
        data.append((x[i], 0))
    return(data)
```

Przykładowe wywołanie:

```{python}
II(1, 2, 4, 10)
```

### Losowe (niezależne)

Ten typ danych może zostać ocenzurowany w losowej chwili $C_i$, zwaną zmienną cenzurującą o rozkładzie $g$ i dystrybuancie $G$ (nie musi być taki sam jak dla zmiennej cenzurowanej). Jest on taki sam dla każdej jednostki $X_i$ i niezależny od niej.

$$
T_i =
\begin{cases}
X_i, & X_i \le C_i \quad (\delta_i = 1),\\[3pt]
C_i, & X_i > C_i \quad (\delta_i = 0),
\end{cases}
\quad
\text{czyli} \quad
T_i = \min(X_i,\, C_i).
$$

Zmienne losowe $C_i$ będziemy losować z rozkładu wykładniczego $\mathcal{E} \left( \eta \right)$ o wartości oczekiwanej równej $\eta$. Łatwo zauważyć, że $\mathcal{E} \left( \eta \right) = \mathcal{GE} \left( \frac{1}{\eta}, 1 \right) = \mathcal{EW} \left( 1, \eta, 1 \right)$.

```{python}
def random(alpha, lamba, theta, n):
    data = []
    x_i = []
    c_i = []
    for _ in range(n):
        x = quantile(np.random.uniform(), alpha, lamba)
        c = quantile(np.random.uniform(), 1, theta)
        if x <= c: data.append((x, 1))
        else: data.append((c, 0))
        x_i.append(x)
        c_i.append(c)
    return(data)
```

Przykładowe wywołanie:

```{python}
random(1, 2, 2, 10)
```

## Zadanie 2

Opisujemy statystyki poszczególnych danych cenzurowanych, odpuszczamy odchylenie standardowe oraz średnią, dodajemy natomiast informację na temat ilości danych cenzurowanych.

```{python}
#| echo: false
def statistics(data):
    
    censored_data = 0
    observations = []
    
    for i in range(len(data)):
        if data[i][1] == 0: censored_data += 1
        observations.append(data[i][0])

    df = pd.DataFrame({
        "Statystyka": ["Minimum", "Kwartyl dolny", "Mediana", "Kwartyl górny", "Maksimum", "Rozstęp międzykwartylowy", "Rozstęp min-max", "Ilość danych ocenzurowanych"],
        "data wygenerowane": [min(observations), np.quantile(observations, 0.25), np.quantile(observations, 0.5), np.quantile(observations, 0.75), max(observations), np.quantile(observations, 0.75) - np.quantile(observations, 0.25), max(observations) - min(observations), censored_data]
    }).round(3)

    return(df)
```

### I. typ

```{python}
#| echo: false
#| tbl-cap: jakaś nazwa1

df = statistics(I(1, 2, 0.5, 10))

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

### II. typ

```{python}
#| echo: false
#| tbl-cap: jakaś nazwa2

df = statistics(II(1, 2, 4, 10))

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

### Losowe (niezależne)

```{python}
#| echo: false
#| tbl-cap: jakaś nazwa3

df = statistics(random(1, 2, 2, 10))

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

## Zadanie 3

W tym zadaniu będziemy wyznaczać statystyki opisowe danych cenzurowanych I-go typu dotyczących leczenia dwoma różnymi lekami A i B. Grupa 40 pacjentów została podzielona losowo na dwie równoliczne podgrupy. Przez jeden rok obserwacji, jednej z nich podawano lek A, drugiej – lek B, i obserwowano czas do remisji choroby.

W grupie otrzymującej lek A uzyskano następujące dane. U dziesięciu pacjentów remisja choroby
nastąpiła w chwilach: 0.03345514, 0.08656403, 0.08799947, 0.24385821, 0.27755032,
0.40787247, 0.58825664, 0.64125620, 0.90679161, 0.94222208, natomiast u pozostałych dziesięciu pacjentów w ciągu roku nie zaobserwowano remisji.

W grupie otrzymującej lek B uzyskano następujące dane. U dziesięciu pacjentów remisja choroby
nastąpiła w chwilach: 0.03788958, 0.12207257, 0.20319983, 0.24474299, 0.30492413,
0.34224462, 0.42950144, 0.44484582, 0.63805066, 0.69119721, natomiast u pozostałych dzisięciu pacjentów w ciągu roku nie zaobserwowano remisji.

```{python}
#| echo: false
#| tbl-cap: jakaś nazwa4

A = [0.03345514, 0.08656403, 0.08799947, 0.24385821, 0.27755032, 0.40787247, 0.58825664, 0.64125620, 0.90679161, 0.94222208]
A = [(A[i], 1) for i in range(len(A))] + [(1, 0)] * 10

B = [0.03788958, 0.12207257, 0.20319983, 0.24474299, 0.30492413, 0.34224462, 0.42950144, 0.44484582, 0.63805066, 0.69119721]
B = [(B[i], 1) for i in range(len(B))] + [(1, 0)] * 10

censored_A = 0
observations_A = []
for i in range(len(A)):
    if A[i][1] == 0: censored_A += 1
    observations_A.append(A[i][0])

censored_B = 0
observations_B = []
for i in range(len(B)):
    if B[i][1] == 0: censored_B += 1
    observations_B.append(B[i][0])

df = pd.DataFrame({
    "Statystyka": ["Minimum", "Kwartyl dolny", "Mediana", "Kwartyl górny", "Maksimum", "Rozstęp międzykwartylowy", "Rozstęp min-max", "Ilość danych ocenzurowanych"],
    "Lek A": [min(observations_A), np.quantile(observations_A, 0.25), np.quantile(observations_A, 0.5), np.quantile(observations_A, 0.75), max(observations_A), np.quantile(observations_A, 0.75) - np.quantile(observations_A, 0.25), max(observations_A) - min(observations_A), censored_A],
    "Lek B": [min(observations_B), np.quantile(observations_B, 0.25), np.quantile(observations_B, 0.5), np.quantile(observations_B, 0.75), max(observations_B), np.quantile(observations_B, 0.75) - np.quantile(observations_B, 0.25), max(observations_B) - min(observations_B), censored_B]
}).round(3)

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

# Lista 3

Tutaj zajmujemy się szacowaniem wartości parametrów rozkładów cenzurowanych, jak i ustalaniem przedziałów ufności.

## Zadanie 1

Zajmujemy się zbiorem pacjentów z lista 2 zadanie 3, uznajemy że wartości pochodzą z rozkładu wykładniczego $\mathcal{E}\left(1\right)$, a dane są zbierane przez jeden rok.

### a) Estymacja punktowa

Pierwszy estymator określamy wzorem $\hat{\theta} = \frac{R}{T_1}$, gdzie:

$$
R = \sum^n_{i=1} \mathbf{1} \left( X_i \leq t_0 \right), \quad T_1 = \sum^R_{i=1} X_{\left( i \right)} + t_0 (n-R)
$$

```{python}
t0 = 1
R = 0
T1 = 0
for i in A:
    if i[1] == 1:
        R += 1
        T1 += i[0]
T1 += t0 * (len(A)-R)
theta_1_A = R/T1
```

Dla danych ze zbioru A otrzymujemy wartość $\hat{\theta}_A$ = `{python} theta_1_A`.

```{python}
t0 = 1
R = 0
T1 = 0
for i in B:
    if i[1] == 1:
        R += 1
        T1 += i[0]
T1 += t0 * (len(B)-R)
theta_1_B = R/T1
```

Dla danych ze zbioru B otrzymujemy wartość $\hat{\theta}_B$ = `{python} theta_1_B`.

### b) Estymacja przedziałowa

Teraz przedział, stworzenie funkcji centralnej na podstawie $T_1$ - zmiennej ciagłej i $R$ - zmiennej dyskretnej jest przejebane, robimy zatem w oparciu o drugi, inny estymator.

Drugi estymator natomiast określamy $\tilde{\theta}=-\frac{log \left( 1-\frac{R}{n}\right)}{t_0}$. Wiedząc, że R ~ $\mathcal{B}\left( n, F_\theta \left( t_0\right) \right)$, szacujemy $p$ jako $\tilde{p}=\frac{R}{n}=1-exp(- \theta t_0)$ i rozwiązujemy względem $\theta$.

Szacujemy przedział $\left[ T_L;T_U\right]$ ufności parametru $p$ dla próby o n próbach i k sukcesach, po czym przekształcając na $\theta$ otrzymujemy wzór na przedział ufności $\left[ \widetilde{T_L};\widetilde{T_U}\right]$:

$$
\widetilde{T_L}=-\frac{log\left(1-T_L\right)}{t_0}, \quad \widetilde{T_U}=-\frac{log\left(1-T_U\right)}{t_0}.
$$

Przedział ufności parametru $p$ na poziomie ufności $1-\alpha$ wyznaczymy za pomocą przedziału Cloppera-Pearsona. Określa się go jako $S_{\leq} \ \cap \ S_{\geq}$, lub równoważenie $\left( \inf S_{\geq} ; \sup S_{\leq} \right)$, gdzie:

$$
\begin{aligned}
S_{\leq} = \{ p: \mathbb{P}\left(\mathcal{B(n,p) \leq k} \right) > \frac{\alpha}{2} \}, \\
S_{\geq} = \{ p: \mathbb{P}\left(\mathcal{B(n,p) \geq k} \right) > \frac{\alpha}{2} \}.
\end{aligned}
$$

**UWAGA**:

1. ze względu na błędy numeryczne, wynik będzie jedynie przybliżony,
2. ustalamy $\alpha=0.05$, zatem przedział będzie na poziomie ufności 95%.

```{python}
def binomial_probability(n, k, p):
    probability = 0
    for i in range(k[0], k[1]+1):
        probability += comb(n, i) * p**i * (1-p)**(n-i)
    return(probability)
  
def interval(n, k, alpha, step0 = 0.1, tol=1e-6):
    alpha = alpha/2
    
    TU = 0
    step = step0
    TU_probability = binomial_probability(n, (0, k), TU)
    
    while abs(TU_probability - alpha) > tol:
        if TU_probability > alpha: TU += step
        else:
            step /= 2
            TU -= step
        TU_probability = binomial_probability(n, (0, k), TU)

    TL = 1
    step = step0
    TL_probability = binomial_probability(n, (k, n), TL)
    
    while abs(TL_probability - alpha) > tol:
        if TL_probability > alpha: TL -= step
        else:
            step /= 2
            TL += step
        TL_probability = binomial_probability(n, (k, n), TL)

    return((TL, TU))
```

```{python}
#| echo: false

alpha = 0.05

TL, TU = interval(20, 10, alpha)
TL, TU = -log(1-TL)/1, -log(1-TU)/1
```

Jako iż liczba sukcesów w obu próbach jest taka sama, to otrzymujemy wspólny przedział ufności na poziomie $1-0.05=0.95$ wynosi (`{python} TL`; `{python} TU`) $\ni \hat{\theta}_A$, ale $\hat{\theta}_B$ juz wypada (z prawej strony).

## Zadanie 2

To samo co przedtem, ale dane cenzurowane drugiego typu. Mamy $n=20$ próbek, oraz kończymy mierzenie czasu po otrzymaniu $m=10$ wyników. Nasz estymator: $\hat{\theta}=\frac{m}{T_2}$, gdzie:

### a) Estymacja punktowa

$$
T_2=\sum^m_{i=n}X_{\left(i\right)} + (n-m)X_{\left(m\right)}.
$$

```{python}
T2A = 0
uncensored = 0
X_m = 0
for i in A:
    if i[1] == 1:
        uncensored += 1
        if X_m < i[0]: X_m = i[0]
        T2A += X_m
T2A += (len(A)-uncensored) * X_m
theta_2_A = uncensored/T2A
```

Dla A wartość parametru to $\hat{\theta}_A$ = `{python} theta_2_A`.

```{python}
T2B = 0
uncensored = 0
X_m = 0
for i in B:
    if i[1] == 1:
        uncensored += 1
        if X_m < i[0]: X_m = i[0]
        T2B += X_m
T2B += (len(B)-uncensored) * X_m
theta_2_B = uncensored/T2B
```

Dla B wartość parametru to $\hat{\theta}_B$ = `{python} theta_2_B`.

### b) Estymacja przedziałowa

Wiemy, że $\frac{\theta \ T_2}{m} \sim \mathcal{G}\left( m, \frac{1}{m} \right)$. Nie ma analitycznego wzoru na dystrybuantę rozkładu gamma, bo jest on postaci $F(t; k, \theta) = \frac{\gamma(k, \frac{x}{\theta})}{\Gamma \left(k \right)}$, gdzie:

$$
\begin{aligned}
\gamma(z, x) = \int_0^x t^{z-1}e^{-t} \,dt - \text{niekompletna funkcja gamma,} \\
\Gamma(z) = \int_0^\infty t^{z-1}e^{-t} \,dt - \text{funkcja gamma,}
\end{aligned}
$$

tym samym nie ma wzoru na funkcję kwantylową, będziemy przybliżać właśnie tą dsytrybuantą.

**UWAGA**: 

1. wartość całki będzie przybliżana metodą parabol (zasada Simpsona) zastosowana w dodatkowym zadaniu z listy 3,
2. w naszym przypadku k to liczba zaobserowanych przypadków $\in \mathbb{N}$, tym samym skorzystamy z właśności $\Gamma\left(n\right)=\left(n+1\right)! \quad \forall \ n \in \mathbb{N}$,
3. poziom ufności $\alpha=0.05$.

```{python}
def incomplete_gamma(s, x):
    n = floor(x/0.001)
    if n % 2 == 1: n += 1
    return(simpson_rule(0, x, lambda t: t**(s-1) * e**(-t), n))
  
def distribution(alpha, theta, t):
  return(incomplete_gamma(alpha, t/theta)/factorial(alpha-1))

def quantile(alpha, theta, p, tol=1e-8):
    a = 0
    b = 1
    while distribution(alpha, theta, b) < p: b += 5

    m = (a+b)/2
    distr_m = distribution(alpha, theta, m)
    while abs(distr_m - p) > tol:
        if distr_m < p: a = m
        else: b = m
        m = (a+b)/2
        distr_m = distribution(alpha, theta, m)
    
    return(m)
```

```{python}
#| echo: false

alpha = 0.05

TL_A = uncensored * quantile(uncensored, 1/uncensored, alpha/2) / T2A
TU_A = uncensored * quantile(uncensored, 1/uncensored, 1-alpha/2) / T2A

TL_B = uncensored * quantile(uncensored, 1/uncensored, alpha/2) / T2B
TU_B = uncensored * quantile(uncensored, 1/uncensored, 1-alpha/2) / T2B
```

Dla A otrzymujemy przedział (`{python} TL_A`; `{python} TU_A`) $\ni \hat{\theta}_A$.
Dla B otrzymujemy przedział (`{python} TL_B`; `{python} TU_B`) $\ni \hat{\theta}_B$.

## Zadanie 3

Generujemy dane cenzurowane I. typu z rozkładu wykładniczego $\mathcal{E}\left(\theta\right)$ i szacujemy dwa estymatory parametru $\hat{\theta}=\frac{R}{T_1}$ oraz $\tilde{\theta}=-\frac{log\left( 1-\frac{R}{n} \right)}{t_0}$. A potem liczymy błąd średniokwadratowy i obciążenie względem rzeczywistego $\theta$.

**UWAGA**:

1. dla "stabilności" estymatorów oraz statystyk, będziemy dla danych $n$, $t_0$ oraz $\theta$ liczyć średnie wartości dla 10000 iteracji,
2. ustalamu $\theta$=1,
3. jeżeli $R=n$, to estymator $\tilde{\theta}$ jest bezsensowny, zatem w takim przypadku dana iteracja będzie powtarza,
4. wartości $\hat{\theta}$ oraz $\tilde{\theta}$ będą wyznaczane na podstawie tych samych wygenerowanych wartości.

```{python}
def quantile(p, theta):
    return(-log(1-p)/theta)

def I(theta, t0, n):
    x = np.array([quantile(np.random.uniform(), theta) for _ in range(n)])
    dane = []
    cenzura = 0
    for i in range(n):
        if x[i] <= t0: 
            dane.append((x[i], 1))
            cenzura += 1
        else: dane.append((t0, 0))
    return((dane, cenzura))
  
def estimation(theta, t0, n, M=1e5):
    theta_1 = []     # z daszkiem
    theta_2 = []    # falowane
    
    iteracje = 0
    
    while iteracje < M:
        
        data = I(theta, t0, n)
        
        if data[1] == n: continue
        else: data = data[0]

        R = 0
        T_1 = 0
        for i in data:
            if i[1] == 1:
                R += 1
                T_1 += i[0]
        T_1 += t0*(n-R)

        theta_1.append(R/T_1)
        theta_2.append(-log(1-R/n)/t0)

        iteracje += 1
        
    return((np.array(theta_1), np.array(theta_2)))
```

```{python}
#| echo: false

def bias_mse(theta, t0, n):
    theta_1, theta_2 = estimation(theta, t0, n)
    df = pd.DataFrame({
        "Estymatory": ["Obciążenie", "Błąd średniokwdratowy"],
        r"$\hat{\theta}$": [np.mean(theta_1-theta), np.mean((theta_1-theta)**2)],
        r"$\tilde{\theta}$": [np.mean(theta_2-theta), np.mean((theta_2-theta)**2)]
    })
    return(df)
```

```{python}
#| echo: false
#| tbl-cap: coś1

df = bias_mse(1, 0.5, 10)

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

```{python}
#| echo: false
#| tbl-cap: coś2

df = bias_mse(1, 0.5, 30)

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

```{python}
#| echo: false
#| tbl-cap: coś3

df = bias_mse(1, 1, 10)

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

```{python}
#| echo: false
#| tbl-cap: coś4

df = bias_mse(1, 1, 30)

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

```{python}
#| echo: false
#| tbl-cap: coś5

df = bias_mse(1, 2, 10)

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```

```{python}
#| echo: false
#| tbl-cap: coś6

df = bias_mse(1, 2, 30)

Markdown( 
  tabulate(df, showindex=False, 
           headers=df.columns))
```